\title{\textbf{Extension} of CTR}
\author{
        Valerio Perrone \\
}
\date{\today}

\documentclass[12pt]{article}

\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{bm}
%\setlength\parindent{0pt}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
%\usepackage{MnSymbol}
\usepackage{float}
%\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[a4paper, total={6in, 8in}]{geometry}



\begin{document}
\maketitle


\section{Model}
Assume there are $K$ topics $\beta := \beta_{1:K}$ and that these are the same for users and items. 


\begin{enumerate}
\item For each user $i$,
\begin{enumerate}
\item  Draw topic proportions  $\theta^u_i \sim$ Dirichlet($\alpha$).
\item  Draw item latent offset  $\epsilon^u_i \sim$ N($0, \lambda_u^{-1} I_k$) and set the item latent vector as $\bm{u_i = \epsilon^u_i + \theta^u_i}$.
\item  For each word $w^u_{in}$.
\begin{enumerate}
\item  Draw topic assignment $z^u_{im} \sim$ Mult($\theta^u$).
\item  Draw word $w^u_{im} \sim$ Mult($\beta_{z^u_{im}}$).
\end{enumerate}
\end{enumerate}


\item For each item $j$,
\begin{enumerate}
\item  Draw topic proportions  $\theta^v_j \sim$ Dirichlet($\alpha$).
\item  Draw item latent offset  $\epsilon^v_j \sim$ N($0, \lambda_v^{-1} I_k$) and set the item latent vector as $v_j = \epsilon^v_j + \theta^v_j$.
\item  For each word $w^v_{jn}$.
\begin{enumerate}
\item  Draw topic assignment $z^v_{jn} \sim$ Mult($\theta$).
\item  Draw word $w^v_{jn} \sim$ Mult($\beta_{z_{jn}}$).
\end{enumerate}
\end{enumerate}

\item For each user-item pair $(i,j)$, draw the rating $r_{ij} \sim$ N($u^T_i v_j, c^{-1}_{ij}$).
\end{enumerate}




\section{Inference}
This is an EM-style algorithm to learn MAP estimates. The goal is to maximize the complete log likelihood

\begin{align*}
L &= - \frac{\lambda_u}{2} \sum_i \bm{(u_i - \theta^u_i)^T(u_i - \theta^u_i)} -\frac{\lambda_v}{2} \sum_j (v_j - \theta^v_j)^T(v_j - \theta^v_j) \\ 
& \bm{+ \sum_i \sum_m log( \sum_k \theta^u_{ik} \beta_{k,w^u_{im}})}  + \sum_j \sum_n log( \sum_k \theta^v_{jk} \beta_{k,w^v_{jn}}) - \sum_{i,j} \frac{c_{i,j}}{2} (r_{ij} - u^T_{i} v_{j})^2.
\end{align*}



The updates are the following:

\begin{enumerate}

\item Given $\theta^v_j,\theta^u_i$  $\rightarrow$ update $u_i$ and $v_j$:
\begin{align*}
u_i &\leftarrow (VC_i V^T + \lambda_u I_k)^{-1} (V C_i R_i \bm{+ \lambda_u \theta^u_i}) \\
v_i &\leftarrow (UC_j U^T + \lambda_v I_k)^{-1} (U C_j R_j + \lambda_v \theta^v_j)
\end{align*}

where $C_i$ is the diagonal matrix with $c_{ij}$ on the diagonal and $R_i = (r_{ij})_{j=1}^J$ for user $i$.


\item Given $U, V$  $\rightarrow$ update $\theta^v_j$ and $\theta^u_i$ by projection gradient using the following\footnote{After defining $\phi_{jnk}$ as the probability that word $z_{jn}$ (i.e. the $n$th word in document $j$) is assigned to topic $k$.}:

\begin{align*}
L(\theta^v_j) &\geq - \frac{\lambda_v}{2} (v_j - \theta^v_j)^T(v_j - \theta^v_j) + \sum_n \sum_k \phi^v_{jnk}(log\theta^v_{jk}\beta_{k,w^v_{jn}} - log \phi^v_{jnk})  \\
\bm{L(\theta^u_i)} &\bm{\geq - \frac{\lambda_u}{2} (u_i - \theta^u_i)^T(u_i - \theta^u_i) + \sum_m \sum_k \phi^u_{ink}(log\theta^u_{ik}\beta_{k,w^u_{im}} - log \phi^u_{ink})}
\end{align*}

given that the optimal $\phi^v_{jnk}$, $\phi^u_{imk}$ and $\beta$ satisfy:

\begin{align*}
\phi^v_{jnk} &\propto \theta^v_{jk} \beta_{k,w^v_{jn}} \\
\bm{\phi^u_{imk}} &\propto \bm{\theta^u_{ik} \beta_{k,w^u_{im}}} \\
\bm{\beta_{kw}} &\bm{\propto \sum_j \sum_i \sum_n \sum_m \phi_{ijnmk} 1[w_{ijnm} = w].}
\end{align*}



\end{enumerate}












\end{document}